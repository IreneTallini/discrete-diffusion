defaults:
  - graph_generator: random_tree
  - gnn_embedder: GATConv
  - _self_

data:
  _target_: discrete_diffusion.data.datamodule.SyntheticGraphDataModule

  graph_generator: ${nn.graph_generator}

  datasets:
    train:
      _target_: discrete_diffusion.data.graph_dataset.GraphDataset

    val:
      _target_: discrete_diffusion.data.graph_dataset.GraphDataset

    test:
      _target_: discrete_diffusion.data.graph_dataset.GraphDataset

  gpus: ${train.trainer.gpus}
  overfit: False

  num_workers:
    train: 16
    val: 4
    test: 4

  batch_size:
    train: 32
    val: 1 # Non funziona con batch size maggiore
    test: 1 # Non funziona con batch size maggiore

  val_percentage: 0.1


module:
  _target_: discrete_diffusion.pl_modules.stochastic_pl_module.StochasticPLModule
  batch_size: ${nn.data.batch_size}
  num_nodes_samples: -1 #-1 random, n > 0 fix
  margin: 1.
  dataset_name: ${nn.graph_generator.graph_type}

  model:
    _target_: discrete_diffusion.modules.autoencoder.autoencoder_stochastic_decoder.AutoencoderStochasticDecoder
    enc_channels: 128
    latent_channels: 128
    dec_channels: 128
    max_num_nodes: ${nn.graph_generator.nx_params.n}
    node_embedder:
      _target_: discrete_diffusion.modules.connectivity_augment.node_embedder.NodeEmbedder
      feature_dim: ???
      num_layers_pre_post_mlp: 2
      hidden_dim_shared: 64
      embedding_dim: ${nn.module.model.enc_channels}
      use_batch_norm_pre_post_mlp: true
      num_convs: 2
      dropout_rate: 0.2
      do_preprocess: true
      jump_mode: cat
      gnn: ${nn.gnn_embedder}

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0 # min value for the lr
    last_epoch: -1
    verbose: False
